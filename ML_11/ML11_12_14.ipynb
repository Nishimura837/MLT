{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model\n",
    "from keras.preprocessing .image import ImageDataGenerator\n",
    "from keras.layers import Input, Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "import datetime\n",
    "import optuna\n",
    "from sklearn.model_selection import KFold\n",
    "from optuna.samplers import TPESampler, GridSampler\n",
    "from sklearn.manifold import TSNE\n",
    "from keras.callbacks import TensorBoard\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 10\n",
    "n_trials = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print(X_train.shape)\n",
    "\n",
    "#ラベルの設定\n",
    "labels = np.array([\n",
    "    'airplane',  #飛行機\n",
    "    'automobile',#バイク\n",
    "    'bird',      #鳥\n",
    "    'cat',       #猫\n",
    "    'deer',      #鹿\n",
    "    'dog',       #犬\n",
    "    'frog',      #カエル\n",
    "    'horse',     #馬\n",
    "    'ship',      #船\n",
    "    'truck'      #トラック\n",
    "    ])\n",
    "\n",
    "#画像の表示のための関数\n",
    "def image_show(x, y, labels):\n",
    "    for i in range(30):\n",
    "        plt.subplot(5, 6, i+1)\n",
    "        #軸を表示しない\n",
    "        plt.axis(\"off\")\n",
    "        #タイトルをラベルの名前で表示\n",
    "        plt.title(labels[y[i][0]])\n",
    "        #表示\n",
    "        plt.imshow(x[i])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return    \n",
    "\n",
    "# トレーニングデータの画像とラベルをセットで表示してみる\n",
    "image_show(X_train, y_train, labels)\n",
    "image_show(X_test, y_test, labels)\n",
    "\n",
    "# データの前処理\n",
    "# ラベルをバイナリクラスにする(yの値を10この数値の配列に変換している)\n",
    "categorical_y_train = to_categorical(y_train, 10)\n",
    "categorical_y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# 正解ラベルの中身の種類 (0~9)をlistに格納\n",
    "class_list = np.unique(y_train).tolist()\n",
    "num_class = len(class_list)\n",
    "print(\"num_class:\", num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageDataGeneratorクラスの作成\n",
    "augmentation_train_datagen = ImageDataGenerator(\n",
    "    rotation_range=10,          # 回転\n",
    "    horizontal_flip=True,       # 左右反転\n",
    "    height_shift_range=0.2,     # 上下平行移動\n",
    "    width_shift_range=0.2,      # 左右平行移動\n",
    "    zoom_range=0.2,             # ランダムにズーム\n",
    "    channel_shift_range = 0.2   # チャンネルシフト\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(trial):\n",
    "    # モデル作成時のパラメータを設定\n",
    "    # モデル作成時のパラメータを設定\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 5)\n",
    "    units = trial.suggest_int('n_units', 8, 256, step=8)\n",
    "    kernel = 3\n",
    "    pool = 2\n",
    "    drop = trial.suggest_float('drop', 0.1, 0.75, step=0.05)\n",
    "\n",
    "    inputs = Input(shape=(32, 32, 3))\n",
    "    x = Conv2D(64, (3,3), padding = \"same\", activation=\"relu\")(inputs)\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        x = Conv2D(units/2**(i-1), (kernel,kernel), padding=\"same\", activation=\"relu\")(x)\n",
    "        x = MaxPooling2D(pool_size=(pool,pool))(x)\n",
    "        x = Dropout(drop)(x)\n",
    "\n",
    "    intermediate_layer = Conv2D(32, (kernel,kernel), padding=\"same\", activation=\"relu\")(x)\n",
    "\n",
    "    # 平滑化\n",
    "    x = Flatten()(intermediate_layer)\n",
    "\n",
    "    # 全統合\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = Dropout(0.6)(x)\n",
    "    predictions = Dense(num_class, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                    optimizer=\"adam\",\n",
    "                    metrics=[\"accuracy\"]\n",
    "                    )\n",
    "    \n",
    "    intermediate_layer_model = Model(inputs=inputs, outputs=intermediate_layer)\n",
    "    \n",
    "    return model, intermediate_layer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # モデルの作成\n",
    "    model, intermediate_layer_model = create_model(trial)\n",
    "\n",
    "    # batch_size範囲を指定\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 256, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 2, 100, step=7)\n",
    "\n",
    "    # KFold のオブジェクトを作成\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    print(\"num_X_train:\", len(X_train))\n",
    "    \n",
    "    accuracies = []\n",
    "    for train_index, valid_index in kf.split(X_train, categorical_y_train):\n",
    "        X_train_subset = X_train[train_index]\n",
    "        y_train_subset = categorical_y_train[train_index]\n",
    "        X_valid_subset = X_train[valid_index]\n",
    "        y_valid_subset = categorical_y_train[valid_index]\n",
    "\n",
    "\n",
    "        # データ拡張を適用\n",
    "        train_generator = augmentation_train_datagen.flow(X_train_subset, y_train_subset,batch_size=batch_size)\n",
    "        valid_generator = augmentation_train_datagen.flow(X_valid_subset, y_valid_subset, batch_size=batch_size)\n",
    "\n",
    "        # 学習\n",
    "        model.fit(train_generator,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=valid_generator,\n",
    "                    callbacks=[EarlyStopping(patience=10)]\n",
    "        )\n",
    "\n",
    "        # 予測\n",
    "        y_pred_subset = model.predict(X_valid_subset)\n",
    "\n",
    "        # 予測したものを元のカテゴリクラスに変換する\n",
    "        y_pred_subset = np.argmax(y_pred_subset, axis=1)\n",
    "        y_valid_subset = np.argmax(y_valid_subset, axis=1)\n",
    "\n",
    "        # 正解数をカウント\n",
    "        num_correct = np.sum(y_pred_subset == y_valid_subset)\n",
    "        print(\"num_correct:\", num_correct)\n",
    "\n",
    "        # valid_index の数\n",
    "        num_valid_samples = len(y_valid_subset)\n",
    "        print(\"num_valid_samples:\", num_valid_samples)\n",
    "\n",
    "        # 正解率\n",
    "        accuracy = num_correct / num_valid_samples\n",
    "        print(\"accuracy:\", accuracy)\n",
    "\n",
    "        # 正解率をリストにappend\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    # CVのaccuracyの平均値を目的関数として返す\n",
    "    print(np.mean(accuracies))\n",
    "    return np.mean(accuracies)\n",
    "\n",
    "\n",
    "# ベイズ最適化\n",
    "sampler = TPESampler()\n",
    "study = optuna.create_study(sampler=sampler, direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=n_trials)\n",
    "# 最適なパラメータの表示\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f'    {key}: {value}')\n",
    "\n",
    "model, intermediate_layer_model = create_model(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, to_file=\"/home/gakubu/デスクトップ/ML_git/MLT/ML_11/model.png\", \n",
    "                        show_shapes=True, show_layer_activations=\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard を使用\n",
    "log_dir = \"/home/gakubu/デスクトップ/ML_git/MLT/ML_11/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2**study.best_trial.params['exponentB']\n",
    "epochs = study.best_trial.params['epochs']\n",
    "# データ拡張を適用\n",
    "train_generator = augmentation_train_datagen.flow(X_train, categorical_y_train,batch_size=batch_size)\n",
    "valid_generator = augmentation_train_datagen.flow(X_test, categorical_y_test, batch_size=batch_size)\n",
    "\n",
    "# 学習\n",
    "history = model.fit(train_generator,\n",
    "                steps_per_epoch=len(train_generator[0])/ batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=valid_generator,\n",
    "                callbacks=[tensorboard_callback, EarlyStopping(patience=10)]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータに対する予測\n",
    "y_test_pred = model.predict(X_test)\n",
    "# 予測したものを元のカテゴリクラスに変換する\n",
    "y_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "y_test_pred_labels = y_test_pred.reshape(-1, 1)\n",
    "# データの形を確認\n",
    "print(\"y_test_pred_labels:\", y_test_pred_labels)\n",
    "print(\"y_test:\", y_test)\n",
    "# 予測と正解ラベルが異なる画像のインデックスを取得\n",
    "misclassified_indices = np.where(y_test_pred != np.argmax(categorical_y_test, axis=1))[0]\n",
    "print(\"misclassified_indices:\", misclassified_indices)\n",
    "# 表示する画像の枚数\n",
    "num_images_to_display = min(50, len(misclassified_indices))\n",
    "num_miss_categorized = len(misclassified_indices)\n",
    "print(\"Num miss categorized:\", num_miss_categorized)\n",
    "# グリッドの行数と列数を計算\n",
    "num_rows = int(np.sqrt(num_images_to_display))\n",
    "num_cols = int(np.ceil(num_images_to_display / num_rows))\n",
    "# 誤分類された画像を表示\n",
    "for i in range(num_images_to_display):\n",
    "    index = misclassified_indices[i] \n",
    "\n",
    "    plt.subplot(num_rows, num_cols, i+1)\n",
    "    plt.imshow(X_test[index])\n",
    "    plt.axis(\"off\")\n",
    "    titles = labels[y_test_pred_labels[index][0]] + \",\" + labels[y_test[index][0]]\n",
    "    plt.title(titles)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 正解数をカウント\n",
    "num_correct = len(y_test) - num_miss_categorized\n",
    "\n",
    "# valid_index の数\n",
    "num_valid_samples = len(y_test)\n",
    "\n",
    "# 正解率\n",
    "print(\"num_correct:\", num_correct)\n",
    "print(\"num_valid_samples:\", num_valid_samples)\n",
    "accuracy = num_correct / num_valid_samples\n",
    "print(\"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 中間層の出力を取得\n",
    "intermediate_output = intermediate_layer_model.predict(X_train)\n",
    "# t-SNEによる次元削減\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_result = tsne.fit_transform(intermediate_output)\n",
    "print(tsne_result.shape)\n",
    "# プロット\n",
    "scatter = plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=y_train.flatten(), cmap=\"tab10\")\n",
    "plt.legend(*scatter.legend_elements())\n",
    "plt.title('t-SNE Visualization of Convolutional Features')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
